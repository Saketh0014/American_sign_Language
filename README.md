# Sign-Language

A very simple CNN project.

## What I did here
- Created 44 gesture samples using OpenCV. For each gesture, 1200 grayscale images of 50x50 pixels were captured and stored in the gestures/ folder.
- The images were flipped using flip_images.py, doubling the dataset to 2400 images per gesture.
- Learned about CNNs and created a CNN similar to the MNIST classifying model using TensorFlow and Keras.
- Used the trained Keras model on a video stream.
- Stored 44 gestures including 26 alphabets, 10 numbers of American Sign Language, and some other gestures.



## Requirements
- Python 3.x
- TensorFlow 1.5
- Keras
- OpenCV 3.4
- h5py
- pyttsx3
- A good grasp of the above topics along with neural networks.
- A good CPU (preferably with a GPU).
- Patience.

## Installing the requirements
- Start your terminal or cmd depending on your OS.
- If you have an NVIDIA GPU, ensure you have the prerequisites for TensorFlow GPU installation, then run:
  
  ```
  pip install -r requirements_gpu.txt
  ```

- If you do not have a GPU, run:

  ```
  pip install -r requirements_cpu.txt
  ```

## How to use this repo
There is no interactive interface, so you will need to figure out most of the usage yourself and make changes to scripts if needed. Here is a basic guide:

### Creating a gesture
- First, set your hand histogram. You only need to do this if lighting conditions change. Run:

  ```
  python set_hand_hist.py
  ```

- A window "Set hand histogram" will appear with 50 squares (5x10).
- Put your hand in those squares, covering all of them.
- Press 'c'. Another window "Thresh" will appear showing white patches corresponding to your skin color.
- Make sure all squares are covered by your hand.
- If not successful, move your hand slightly and press 'c' again until you get a good histogram.
- Press 's' to save the histogram and close all windows.

- 44 gestures (0-43) are already added. You can add or replace gestures by running:

  ```
  python create_gestures.py
  ```

- Enter the gesture number and name/text.
- A window "Capturing gestures" will appear with a green box to perform your gesture.
- Press 'c' to start capturing. Move your hand slightly during capture.
- You can pause/resume capturing by pressing 'c'.
- After 1200 images are captured, the window closes automatically.

- After capturing gestures, flip images using:

  ```
  python flip_images.py
  ```

- Then run once:

  ```
  python load_images.py
  ```

### Displaying all gestures
- To see all stored gestures in the 'gestures/' folder, run:

  ```
  python display_all_gestures.py
  ```

### Training a model
- Train using TensorFlow:

  ```
  python cnn_tf.py
  ```

- Train using Keras:

  ```
  python cnn_keras.py
  ```

- TensorFlow training outputs checkpoints and metagraph files in tmp/cnn_model3.
- Keras training outputs the model as cnn_model_keras2.h5 in the root directory.
- Retrain only if gestures are added or removed.

### Get model reports
- Ensure test_images and test_labels are generated by load_images.py.
- Run:

  ```
  python get_model_reports.py
  ```

- This provides confusion matrix, f scores, precision, and recall.

### Testing gestures
- The TensorFlow model is not used due to loading overhead.
- Keras model is used for prediction.
- First, set your hand histogram as described above.
- Run:

  ```
  python recognize_gesture.py
  ```

- A small green box will appear where you perform gestures.

### Using fun_util.py
- Set your hand histogram as above.
- Run:

  ```
  python fun_util.py
  ```

- Text Mode (press 't' to switch):
  - Create words using fingerspelling or predefined gestures.
  - Text converts to speech when hand is removed from the green box.
  - Keep the same gesture for 15 frames for recognition.


